import requestsfrom config.config import headers, proxiesfrom requests.exceptions import ProxyError, ConnectTimeout, SSLErrorfrom bs4 import BeautifulSoup as Bsfrom urllib import parseimport timedef get_links(path):    """    :param path: /wiki/<词条名称>    :return: list of all valid wiki links in https://zh.wikipedia.org/wiki/<词条名称>    """    urls = set()    # check if the path parameter was encoded    if '%' not in path:        path = parse.quote(path)    while True:        try:            # requests.exceptions.SSLError: ("bad handshake: SysCallError(54, 'ECONNRESET')",)            # set verify to False can solve it but will throw error            # so make another except expression to catch the SSLError error            page = requests.get('https://zh.wikipedia.org' + path,                                headers=headers, timeout=5, proxies=proxies)            # 注意 get不要有禁止重定向 因为会返回301 定向到：            # 'https://zh.wikipedia.org/wiki/Wikipedia:%E9%A6%96%E9%A1%B5'        except ProxyError:            print('代理超时 重试')        except ConnectTimeout:            print('代理挂了 重试')        except SSLError:            print('证书验证出错 重试')        else:            html = page.text            soup = Bs(html, 'lxml')            time.sleep(3)            for url in soup.find(id='bodyContent').findAll('a'):                href = url.get('href', False)                if href and href.startswith('/wiki') and ':' not in href:                    urls.add(href)            break    return list(urls)